{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1\n",
    "\n",
    "Please find the task 1 completed on the PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The optimal Bellman condition for the states FACEBOOK, CLASS1, CLASS2 and CLASS3:\n",
    "\n",
    "    V*FB = max((-1 + 0.9 * V*FB), (-1 + 0.9 * V*C1))\n",
    "    \n",
    "    V*C1 = max((-1 + 0.9 * V*FB), (-2 + 0.9 * V*C2))\n",
    "    \n",
    "    V*C2 = max((-2 + 0.9 * V*C3), 0)\n",
    "    \n",
    "    V*C3 = max((1 + 0.9 * (0.2 * V*C1 + 0.4 * V*C2 + 0.4 * V*C3)), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use numpy to find the optimal policy using value iteration. \n",
    "   Show all the state values at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE V VALUES AND POLICY VALUES AT ITERATION 1 :\n",
      "[-1. -1.  0. 10.]\n",
      "[0. 0. 3. 4.] \n",
      "\n",
      "THE V VALUES AND POLICY VALUES AT ITERATION 2 :\n",
      "[-1.9 -1.9  7.  10. ]\n",
      "[0. 0. 3. 4.] \n",
      "\n",
      "THE V VALUES AND POLICY VALUES AT ITERATION 3 :\n",
      "[-2.71  4.3   7.   10.  ]\n",
      "[0. 2. 3. 4.] \n",
      "\n",
      "THE V VALUES AND POLICY VALUES AT ITERATION 4 :\n",
      "[ 2.87  4.3   7.   10.  ]\n",
      "[1. 2. 3. 4.] \n",
      "\n",
      "THE V VALUES AND POLICY VALUES AT ITERATION 5 :\n",
      "[ 2.87  4.3   7.   10.  ]\n",
      "[1. 2. 3. 4.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Vs = np.zeros(4)\n",
    "vnew = np.zeros(4)\n",
    "pi = np.zeros(4)\n",
    "theta = 0.00001\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    i = 1+i\n",
    "    vnew[0] = max((-1+0.9*Vs[0]), (-1+0.9*Vs[1]))\n",
    "    pi[0] = np.argmax([(-1+0.9*Vs[0]), (-1+0.9*Vs[1])])\n",
    "    d = abs(vnew[0]-Vs[0])\n",
    "    \n",
    "    vnew[1] = max((-1+0.9*Vs[0]), (-2+0.9*Vs[2]))\n",
    "    pi[1] = np.argmax([(-1+0.9*Vs[0]), -float('inf'), (-2+0.9*Vs[2])])\n",
    "    d = max(d, abs(vnew[1]-Vs[1]))\n",
    "    \n",
    "    vnew[2] = max((-2+0.9*Vs[3]), 0)\n",
    "    pi[2] = np.argmax([-float('inf'), -float('inf'), -float('inf'), (-2+0.9*Vs[2])])\n",
    "    d = max(d, abs(vnew[2]-Vs[2]))\n",
    "    \n",
    "    vnew[3] = max((1+0.9*(0.2*Vs[1]+0.4*Vs[2]+0.4*Vs[3])), 10)\n",
    "    pi[3] = np.argmax([-float('inf'), -float('inf'), -float('inf'), (1+0.9*(0.2*Vs[1]+0.4*Vs[2]+0.4*Vs[3])), 10])\n",
    "    d = max(d, abs(vnew[3]-Vs[3]))\n",
    "    \n",
    "    print('THE V VALUES AND POLICY VALUES AT ITERATION', i, ':')\n",
    "    print(vnew)\n",
    "    print(pi,'\\n')\n",
    "\n",
    "    if (d < theta):\n",
    "        break\n",
    "        \n",
    "    Vs[0] = vnew[0]\n",
    "    Vs[1] = vnew[1]\n",
    "    Vs[2] = vnew[2] \n",
    "    Vs[3] = vnew[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final V values are:\n",
    "\n",
    "    V_FB = 2.87\n",
    "\n",
    "    V_C1 = 4.3\n",
    "\n",
    "    V_C2 = 7\n",
    "\n",
    "    V_C3 = 10\n",
    "\n",
    "\n",
    "The best Policy acquired is:\n",
    "    \n",
    "    pi_FB = quit\n",
    "\n",
    "    pi_C1 = study\n",
    "\n",
    "    pi_C2 = study\n",
    "\n",
    "    pi_C3 = sleep\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rewards = np.array([[-1, -1, -float('inf'), -float('inf'), float('inf')],\n",
    "                    [-1, -float('inf'), -2, -float('inf'), -float('inf')],\n",
    "                    [-float('inf'), -float('inf'), -2, -float('inf'), 0],\n",
    "                    [-float('inf'), -float('inf'), 10, 1, -float('inf')],\n",
    "                   ])\n",
    "Action = np.array([3, 3, 2, 3, 2, 4])\n",
    "State = np.array([3, 3, 2, 3, 1, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(Qs, state, next_state, action):\n",
    "    rsa = Rewards[state, action]\n",
    "    qsa = Qs[state, action]\n",
    "    new_q = rsa + 0.9 * np.nanmax(Qs[next_state, :])\n",
    "    print('THE NEW Q VALUE IS: ', new_q)\n",
    "    Qs[state, action] = float(str(round(new_q, 2)))\n",
    "    return Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE NEW Q VALUE IS:  10.0\n",
      "Q VALUES AFTER ACTION 1 :\n",
      "[[-1.  -1.   nan  nan  nan]\n",
      " [-1.9  nan -2.   nan  nan]\n",
      " [ nan  nan -1.1  nan  0. ]\n",
      " [ nan  nan 10.  10.   nan]] \n",
      "\n",
      "THE NEW Q VALUE IS:  1.0\n",
      "Q VALUES AFTER ACTION 2 :\n",
      "[[-1.  -1.   nan  nan  nan]\n",
      " [-1.9  nan -2.   nan  nan]\n",
      " [ nan  nan -1.1  nan  0. ]\n",
      " [ nan  nan 10.   1.   nan]] \n",
      "\n",
      "THE NEW Q VALUE IS:  7.0\n",
      "Q VALUES AFTER ACTION 3 :\n",
      "[[-1.  -1.   nan  nan  nan]\n",
      " [-1.9  nan -2.   nan  nan]\n",
      " [ nan  nan  7.   nan  0. ]\n",
      " [ nan  nan 10.   1.   nan]] \n",
      "\n",
      "THE NEW Q VALUE IS:  -0.71\n",
      "Q VALUES AFTER ACTION 4 :\n",
      "[[-1.   -1.     nan   nan   nan]\n",
      " [-1.9    nan -2.     nan   nan]\n",
      " [  nan   nan  7.     nan  0.  ]\n",
      " [  nan   nan 10.   -0.71   nan]] \n",
      "\n",
      "THE NEW Q VALUE IS:  4.3\n",
      "Q VALUES AFTER ACTION 5 :\n",
      "[[-1.   -1.     nan   nan   nan]\n",
      " [-1.9    nan  4.3    nan   nan]\n",
      " [  nan   nan  7.     nan  0.  ]\n",
      " [  nan   nan 10.   -0.71   nan]] \n",
      "\n",
      "THE NEW Q VALUE IS:  0.0\n",
      "Q VALUES AFTER ACTION 6 :\n",
      "[[-1.   -1.     nan   nan   nan]\n",
      " [-1.9    nan  4.3    nan   nan]\n",
      " [  nan   nan  7.     nan  0.  ]\n",
      " [  nan   nan 10.   -0.71   nan]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([[-1, -1, np.nan, np.nan, np.nan],\n",
    "             [-1.9, np.nan, -2, np.nan, np.nan],\n",
    "             [np.nan, np.nan, -1.1, np.nan, 0],\n",
    "             [np.nan, np.nan, 10, 1, np.nan],\n",
    "             [0, 0, 0, 0, 0]])\n",
    "\n",
    "for i in range (len(Action)):\n",
    "    state = State[i]\n",
    "    action = Action[i]\n",
    "    next_state = State[i+1]\n",
    "    qnew = update_q(Q, state, next_state, action)\n",
    "    print('Q VALUES AFTER ACTION', i+1, ':')\n",
    "    print(qnew[0:4,:],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final Q values are:\n",
    "      \n",
    "                  FB  quit  study  pub  sleep\n",
    "        \n",
    "     Facebook     -1    -1     -     -     -\n",
    " \n",
    "     Class 1     -1.9    -    4.3    -     -\n",
    "\n",
    "     Class 2       -     -     7     -     0\n",
    " \n",
    "     Class 3       -     -    10   -0.71   -\n",
    " \n",
    " For complete steps please check the PDF file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
